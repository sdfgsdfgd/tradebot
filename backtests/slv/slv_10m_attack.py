#!/usr/bin/env python3
"""Standalone SLV 10m attack runner (temporary exploration script).

Pipeline:
1) Stage A discovery (96 combos): slow-bias + fast-entry core.
2) Stage B discovery (216 combos): aggressive-precise risk layer on Stage A top-6.
3) Promotion: 10y/2y/1y multiwindow with min_trades_per_year=500.

This script intentionally reuses canonical engine paths by invoking:
- `python -m tradebot.backtest spot_multitimeframe ...`
so cache reuse, worker sharding (`--jobs`), and scoring stay identical.
"""

from __future__ import annotations

import argparse
import json
import shlex
import subprocess
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import Iterable


def _utc_now_iso() -> str:
    return datetime.now(tz=timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")


def _merge_dict(base: dict | None, overrides: dict[str, object]) -> dict:
    out = dict(base or {})
    for key, value in overrides.items():
        if value is None:
            out.pop(str(key), None)
        else:
            out[str(key)] = value
    return out


def _blank_metrics() -> dict[str, float | int]:
    return {
        "pnl": 0.0,
        "roi": 0.0,
        "win_rate": 0.0,
        "trades": 0,
        "max_drawdown": 0.0,
        "max_drawdown_pct": 0.0,
        "pnl_over_dd": 0.0,
        "roi_over_dd_pct": 0.0,
    }


def _read_json(path: Path) -> dict:
    return json.loads(path.read_text())


def _write_json(path: Path, payload: dict) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(payload, indent=2, sort_keys=False))


def _groups_count(path: Path) -> int:
    payload = _read_json(path)
    groups = payload.get("groups") if isinstance(payload, dict) else None
    return len(groups) if isinstance(groups, list) else 0


def _parse_window(raw: str) -> tuple[str, str]:
    text = str(raw or "").strip()
    if ":" not in text:
        raise SystemExit(f"Invalid window (expected YYYY-MM-DD:YYYY-MM-DD): {raw!r}")
    start_s, end_s = [part.strip() for part in text.split(":", 1)]
    if len(start_s) != 10 or len(end_s) != 10:
        raise SystemExit(f"Invalid window (expected YYYY-MM-DD:YYYY-MM-DD): {raw!r}")
    return start_s, end_s


def _iter_candidate_groups(payload: dict, *, symbol: str) -> Iterable[tuple[str, dict, dict, dict]]:
    groups = payload.get("groups") if isinstance(payload, dict) else None
    if not isinstance(groups, list):
        return
    symbol_upper = str(symbol).strip().upper()
    for group in groups:
        if not isinstance(group, dict):
            continue
        entries = group.get("entries")
        if not isinstance(entries, list) or not entries:
            continue
        first = entries[0]
        if not isinstance(first, dict):
            continue
        strategy = first.get("strategy")
        metrics = first.get("metrics")
        if not isinstance(strategy, dict) or not isinstance(metrics, dict):
            continue
        if str(strategy.get("symbol") or "").strip().upper() != symbol_upper:
            continue
        filters = group.get("filters")
        if filters is not None and not isinstance(filters, dict):
            continue
        group_name = str(group.get("name") or "")
        yield group_name, strategy, (filters or {}), metrics


def _payload_from_groups(
    *,
    name: str,
    source: str,
    windows: list[dict[str, str]],
    groups: list[dict],
) -> dict:
    return {
        "name": name,
        "generated_at": _utc_now_iso(),
        "source": source,
        "windows": windows,
        "groups": groups,
        "notes": [
            "Generated by backtests/slv/slv_10m_attack.py",
        ],
    }


def _run_and_tee(*, cmd: list[str], cwd: Path, log_path: Path) -> None:
    log_path.parent.mkdir(parents=True, exist_ok=True)
    print(f"$ {shlex.join(cmd)}")
    with log_path.open("w") as logf:
        proc = subprocess.Popen(
            cmd,
            cwd=str(cwd),
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
        )
        assert proc.stdout is not None
        for line in proc.stdout:
            print(line, end="")
            logf.write(line)
        rc = proc.wait()
    if rc != 0:
        raise SystemExit(f"Command failed with exit code {rc}: {shlex.join(cmd)}")


def _run_multitimeframe(
    *,
    repo_root: Path,
    milestones: Path,
    symbol: str,
    bar_size: str,
    use_rth: bool,
    offline: bool,
    cache_dir: Path,
    jobs: int,
    top: int,
    min_trades: int,
    min_win: float,
    windows: list[str],
    write_top: int,
    out_path: Path,
    log_path: Path,
    min_trades_per_year: float | None = None,
    require_positive_pnl: bool = False,
) -> None:
    cmd = [
        sys.executable,
        "-u",
        "-m",
        "tradebot.backtest",
        "spot_multitimeframe",
        "--milestones",
        str(milestones),
        "--symbol",
        str(symbol),
        "--bar-size",
        str(bar_size),
        "--cache-dir",
        str(cache_dir),
        "--jobs",
        str(int(jobs)),
        "--top",
        str(int(top)),
        "--min-trades",
        str(int(min_trades)),
        "--min-win",
        str(float(min_win)),
        "--write-top",
        str(int(write_top)),
        "--out",
        str(out_path),
    ]
    if use_rth:
        cmd.append("--use-rth")
    if offline:
        cmd.append("--offline")
    if require_positive_pnl:
        cmd.append("--require-positive-pnl")
    if min_trades_per_year is not None:
        cmd.extend(["--min-trades-per-year", str(float(min_trades_per_year))])
    for window in windows:
        cmd.extend(["--window", str(window)])
    _run_and_tee(cmd=cmd, cwd=repo_root, log_path=log_path)


def _build_stage_a_groups(
    *,
    symbol: str,
    base_strategy: dict,
    base_filters: dict,
) -> list[dict]:
    ema_presets = ("3/7", "4/9", "5/13", "8/21")
    regime_pockets: tuple[tuple[str, dict[str, object]], ...] = (
        (
            "4h ST(3,0.25,close)",
            {
                "regime_bar_size": "4 hours",
                "supertrend_atr_period": 3,
                "supertrend_multiplier": 0.25,
                "supertrend_source": "close",
            },
        ),
        (
            "4h ST(5,0.2,close)",
            {
                "regime_bar_size": "4 hours",
                "supertrend_atr_period": 5,
                "supertrend_multiplier": 0.2,
                "supertrend_source": "close",
            },
        ),
        (
            "1d ST(7,0.4,close)",
            {
                "regime_bar_size": "1 day",
                "supertrend_atr_period": 7,
                "supertrend_multiplier": 0.4,
                "supertrend_source": "close",
            },
        ),
    )
    regime2_opts: tuple[tuple[str, dict[str, object]], ...] = (
        (
            "off",
            {
                "regime2_mode": "off",
            },
        ),
        (
            "1d ST(7,0.4,close)",
            {
                "regime2_mode": "supertrend",
                "regime2_bar_size": "1 day",
                "regime2_supertrend_atr_period": 7,
                "regime2_supertrend_multiplier": 0.4,
                "regime2_supertrend_source": "close",
            },
        ),
    )
    stops = (0.006, 0.008)
    max_open_values = (1, 2)

    fixed_strategy = {
        "instrument": "spot",
        "symbol": str(symbol).strip().upper(),
        "signal_bar_size": "10 mins",
        "signal_use_rth": True,
        "spot_exec_bar_size": "5 mins",
        "spot_close_eod": True,
        "flip_exit_only_if_profit": True,
        "flip_exit_min_hold_bars": 2,
        "ema_entry_mode": "trend",
        "entry_confirm_bars": 0,
        "regime_mode": "supertrend",
    }
    fixed_filters = {
        "entry_start_hour_et": 9,
        "entry_end_hour_et": 16,
    }

    groups: list[dict] = []
    rank = 0
    for ema in ema_presets:
        for regime_note, regime_over in regime_pockets:
            for regime2_note, regime2_over in regime2_opts:
                for sl in stops:
                    for max_open in max_open_values:
                        rank += 1
                        strategy = dict(base_strategy)
                        strategy.update(fixed_strategy)
                        strategy.update(regime_over)
                        strategy.update(regime2_over)
                        strategy["ema_preset"] = str(ema)
                        strategy["spot_stop_loss_pct"] = float(sl)
                        strategy["max_open_trades"] = int(max_open)
                        if strategy.get("regime2_mode") == "off":
                            strategy.pop("regime2_ema_preset", None)
                        filters = dict(base_filters)
                        filters.update(fixed_filters)
                        note = (
                            f"A ema={ema} regime={regime_note} regime2={regime2_note} "
                            f"sl={sl:g} max_open={max_open}"
                        )
                        groups.append(
                            {
                                "name": f"StageA #{rank:03d} {note}",
                                "filters": filters,
                                "entries": [
                                    {
                                        "symbol": str(symbol).strip().upper(),
                                        "metrics": _blank_metrics(),
                                        "strategy": strategy,
                                    }
                                ],
                                "_eval": {"stage": "A", "note": note},
                            }
                        )
    return groups


def _build_stage_b_groups(
    *,
    symbol: str,
    stage_a_top_payload: dict,
) -> list[dict]:
    seeds = list(_iter_candidate_groups(stage_a_top_payload, symbol=symbol))

    shock_specs: tuple[tuple[int, float, float, str], ...] = (
        (10, 8.0, 0.05, "both"),
        (20, 8.0, 0.05, "both"),
        (10, 10.0, 0.05, "both"),
        (10, 8.0, 0.10, "both"),
    )
    risk_specs: list[tuple[float, float, float]] = []
    for tr_med in (2.75, 3.0, 3.25):
        for tr_delta in (0.25, 0.5):
            for long_factor in (0.2, 0.0):
                risk_specs.append((float(tr_med), float(tr_delta), float(long_factor)))
    short_mults = (0.02, 0.01)

    cross_pairs: tuple[tuple[tuple[int, float, float, str], tuple[float, float, float]], ...] = (
        (shock_specs[0], (2.75, 0.25, 0.2)),
        (shock_specs[1], (3.0, 0.25, 0.2)),
        (shock_specs[2], (3.0, 0.5, 0.0)),
        (shock_specs[3], (3.25, 0.5, 0.0)),
    )

    groups: list[dict] = []
    rank = 0

    def _shock_over(lb: int, target: float, min_mult: float, apply_to: str) -> dict[str, object]:
        return {
            "shock_scale_detector": "daily_drawdown",
            "shock_drawdown_lookback_days": int(lb),
            "shock_risk_scale_target_atr_pct": float(target),
            "shock_risk_scale_min_mult": float(min_mult),
            "shock_risk_scale_apply_to": str(apply_to),
        }

    def _risk_over(tr_med: float, tr_delta: float, long_factor: float) -> dict[str, object]:
        return {
            "riskoff_tr5_med_pct": None,
            "riskpop_tr5_med_pct": None,
            "riskpanic_tr5_med_pct": float(tr_med),
            "riskpanic_neg_gap_ratio_min": 0.6,
            "riskpanic_neg_gap_abs_pct_min": 0.005,
            "riskpanic_lookback_days": 5,
            "riskpanic_tr5_med_delta_min_pct": float(tr_delta),
            "riskpanic_tr5_med_delta_lookback_days": 1,
            "risk_entry_cutoff_hour_et": 15,
            "riskpanic_long_risk_mult_factor": float(long_factor),
            "riskpanic_short_risk_mult_factor": 1.0,
            "riskpanic_long_scale_mode": "linear",
            "riskpanic_long_scale_tr_delta_max_pct": None,
        }

    symbol_upper = str(symbol).strip().upper()
    for seed_idx, (seed_name, seed_strategy, seed_filters, _) in enumerate(seeds, start=1):
        for lb, target, min_mult, apply_to in shock_specs:
            rank += 1
            note = (
                f"B seed#{seed_idx} shock-only lb={lb} target={target:g} "
                f"min={min_mult:g} apply={apply_to}"
            )
            groups.append(
                {
                    "name": f"StageB #{rank:03d} {note}",
                    "filters": _merge_dict(seed_filters, _shock_over(lb, target, min_mult, apply_to)),
                    "entries": [
                        {
                            "symbol": symbol_upper,
                            "metrics": _blank_metrics(),
                            "strategy": dict(seed_strategy),
                        }
                    ],
                    "_eval": {"stage": "B", "seed": seed_name, "note": note, "kind": "shock_only"},
                }
            )

        for tr_med, tr_delta, long_factor in risk_specs:
            for short_mult in short_mults:
                rank += 1
                strategy = dict(seed_strategy)
                strategy["spot_short_risk_mult"] = float(short_mult)
                note = (
                    f"B seed#{seed_idx} riskpanic tr_med={tr_med:g} tr_delta={tr_delta:g} "
                    f"long={long_factor:g} short_mult={short_mult:g}"
                )
                groups.append(
                    {
                        "name": f"StageB #{rank:03d} {note}",
                        "filters": _merge_dict(seed_filters, _risk_over(tr_med, tr_delta, long_factor)),
                        "entries": [
                            {
                                "symbol": symbol_upper,
                                "metrics": _blank_metrics(),
                                "strategy": strategy,
                            }
                        ],
                        "_eval": {"stage": "B", "seed": seed_name, "note": note, "kind": "riskpanic_only"},
                    }
                )

        for (lb, target, min_mult, apply_to), (tr_med, tr_delta, long_factor) in cross_pairs:
            for short_mult in short_mults:
                rank += 1
                strategy = dict(seed_strategy)
                strategy["spot_short_risk_mult"] = float(short_mult)
                over = _shock_over(lb, target, min_mult, apply_to)
                over.update(_risk_over(tr_med, tr_delta, long_factor))
                note = (
                    f"B seed#{seed_idx} cross lb={lb} target={target:g} min={min_mult:g} "
                    f"tr_med={tr_med:g} tr_delta={tr_delta:g} long={long_factor:g} short_mult={short_mult:g}"
                )
                groups.append(
                    {
                        "name": f"StageB #{rank:03d} {note}",
                        "filters": _merge_dict(seed_filters, over),
                        "entries": [
                            {
                                "symbol": symbol_upper,
                                "metrics": _blank_metrics(),
                                "strategy": strategy,
                            }
                        ],
                        "_eval": {"stage": "B", "seed": seed_name, "note": note, "kind": "cross_subset"},
                    }
                )

    return groups


def main() -> None:
    default_seed = (
        "backtests/slv/slv_exec5m_v30_seed_v25_as_10m_rth_top80.json"
    )
    default_out_dir = "backtests/slv"
    default_stage_window = "2024-01-08:2026-01-08"

    ap = argparse.ArgumentParser(description="Standalone SLV 10m attack runner (StageA+StageB+Promotion).")
    ap.add_argument("--symbol", default="SLV")
    ap.add_argument("--seed-milestones", default=default_seed)
    ap.add_argument("--out-dir", default=default_out_dir)
    ap.add_argument("--cache-dir", default="db")
    ap.add_argument("--jobs", type=int, default=0, help="spot_multitimeframe workers (0 = auto).")
    ap.add_argument("--offline", action=argparse.BooleanOptionalAction, default=True)
    ap.add_argument("--run-id", default=None, help="Output prefix. Default uses timestamp.")

    ap.add_argument("--stage-window", default=default_stage_window, help="Discovery window YYYY-MM-DD:YYYY-MM-DD.")
    ap.add_argument("--stage-a-top-k", type=int, default=6)
    ap.add_argument("--stage-a-min-trades", type=int, default=0)
    ap.add_argument("--stage-b-min-trades", type=int, default=0)
    ap.add_argument("--stage-b-write-top", type=int, default=80)

    ap.add_argument(
        "--promotion-window",
        action="append",
        default=None,
        help="Promotion windows (repeatable).",
    )
    ap.add_argument("--promotion-min-trades", type=int, default=0)
    ap.add_argument("--promotion-min-trades-per-year", type=float, default=500.0)
    ap.add_argument("--promotion-write-top", type=int, default=80)
    ap.add_argument("--promotion-require-positive", action="store_true", default=False)
    args = ap.parse_args()

    repo_root = Path(__file__).resolve().parents[2]
    out_dir = (repo_root / str(args.out_dir)).resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    ts = datetime.now(tz=timezone.utc).strftime("%Y%m%d_%H%M%S")
    run_id = str(args.run_id or f"slv_exec5m_hybridquick10_{ts}")
    symbol = str(args.symbol).strip().upper()

    seed_path = (repo_root / str(args.seed_milestones)).resolve()
    if not seed_path.exists():
        raise SystemExit(f"Seed milestones file not found: {seed_path}")
    seed_payload = _read_json(seed_path)
    seed_candidates = list(_iter_candidate_groups(seed_payload, symbol=symbol))
    if not seed_candidates:
        raise SystemExit(f"No seed candidates for {symbol} in {seed_path}")
    _, seed_strategy, seed_filters, _ = seed_candidates[0]

    stage_start, stage_end = _parse_window(str(args.stage_window))
    stage_windows_payload = [{"start": stage_start, "end": stage_end}]
    promotion_windows = list(args.promotion_window or ["2016-01-08:2026-01-08", "2024-01-08:2026-01-08", "2025-01-08:2026-01-08"])
    for window_raw in promotion_windows:
        _parse_window(str(window_raw))

    stage_a_candidates_path = out_dir / f"{run_id}_stageA_candidates.json"
    stage_a_top_path = out_dir / f"{run_id}_stageA_2y_top{int(args.stage_a_top_k)}.json"
    stage_a_log_path = out_dir / f"{run_id}_stageA_2y_log.txt"

    stage_b_candidates_path = out_dir / f"{run_id}_stageB_candidates.json"
    stage_b_top_path = out_dir / f"{run_id}_stageB_2y_top{int(args.stage_b_write_top)}.json"
    stage_b_log_path = out_dir / f"{run_id}_stageB_2y_log.txt"

    promotion_top_path = out_dir / f"{run_id}_promotion_10y2y1y_top{int(args.promotion_write_top)}.json"
    promotion_log_path = out_dir / f"{run_id}_promotion_10y2y1y_log.txt"

    stage_a_groups = _build_stage_a_groups(symbol=symbol, base_strategy=seed_strategy, base_filters=seed_filters)
    if len(stage_a_groups) != 96:
        raise SystemExit(f"Stage A combination count mismatch: expected 96, got {len(stage_a_groups)}")
    _write_json(
        stage_a_candidates_path,
        _payload_from_groups(
            name=f"{run_id}_stageA_candidates",
            source=str(seed_path),
            windows=stage_windows_payload,
            groups=stage_a_groups,
        ),
    )
    print(f"Stage A candidates written: {stage_a_candidates_path} (count={len(stage_a_groups)})")

    _run_multitimeframe(
        repo_root=repo_root,
        milestones=stage_a_candidates_path,
        symbol=symbol,
        bar_size="10 mins",
        use_rth=True,
        offline=bool(args.offline),
        cache_dir=Path(args.cache_dir),
        jobs=int(args.jobs),
        top=len(stage_a_groups),
        min_trades=int(args.stage_a_min_trades),
        min_win=0.0,
        windows=[f"{stage_start}:{stage_end}"],
        write_top=max(1, int(args.stage_a_top_k)),
        out_path=stage_a_top_path,
        log_path=stage_a_log_path,
    )
    stage_a_top_count = _groups_count(stage_a_top_path)
    print(f"Stage A shortlist written: {stage_a_top_path} (count={stage_a_top_count})")
    if stage_a_top_count <= 0:
        raise SystemExit("Stage A produced no shortlisted candidates; aborting Stage B.")

    stage_a_top_payload = _read_json(stage_a_top_path)
    stage_b_groups = _build_stage_b_groups(symbol=symbol, stage_a_top_payload=stage_a_top_payload)
    expected_stage_b = stage_a_top_count * 36
    if len(stage_b_groups) != expected_stage_b:
        raise SystemExit(
            f"Stage B combination count mismatch: expected {expected_stage_b}, got {len(stage_b_groups)}"
        )
    _write_json(
        stage_b_candidates_path,
        _payload_from_groups(
            name=f"{run_id}_stageB_candidates",
            source=str(stage_a_top_path),
            windows=stage_windows_payload,
            groups=stage_b_groups,
        ),
    )
    print(f"Stage B candidates written: {stage_b_candidates_path} (count={len(stage_b_groups)})")

    _run_multitimeframe(
        repo_root=repo_root,
        milestones=stage_b_candidates_path,
        symbol=symbol,
        bar_size="10 mins",
        use_rth=True,
        offline=bool(args.offline),
        cache_dir=Path(args.cache_dir),
        jobs=int(args.jobs),
        top=len(stage_b_groups),
        min_trades=int(args.stage_b_min_trades),
        min_win=0.0,
        windows=[f"{stage_start}:{stage_end}"],
        write_top=max(1, int(args.stage_b_write_top)),
        out_path=stage_b_top_path,
        log_path=stage_b_log_path,
    )
    stage_b_top_count = _groups_count(stage_b_top_path)
    print(f"Stage B shortlist written: {stage_b_top_path} (count={stage_b_top_count})")
    if stage_b_top_count <= 0:
        raise SystemExit("Stage B produced no shortlisted candidates; aborting promotion.")

    _run_multitimeframe(
        repo_root=repo_root,
        milestones=stage_b_top_path,
        symbol=symbol,
        bar_size="10 mins",
        use_rth=True,
        offline=bool(args.offline),
        cache_dir=Path(args.cache_dir),
        jobs=int(args.jobs),
        top=stage_b_top_count,
        min_trades=int(args.promotion_min_trades),
        min_win=0.0,
        windows=[str(w) for w in promotion_windows],
        write_top=max(1, int(args.promotion_write_top)),
        out_path=promotion_top_path,
        log_path=promotion_log_path,
        min_trades_per_year=float(args.promotion_min_trades_per_year),
        require_positive_pnl=bool(args.promotion_require_positive),
    )
    promotion_count = _groups_count(promotion_top_path)
    print(f"Promotion output written: {promotion_top_path} (count={promotion_count})")
    print("")
    print("Artifacts:")
    print(f"- Stage A candidates: {stage_a_candidates_path}")
    print(f"- Stage A shortlist:  {stage_a_top_path}")
    print(f"- Stage A log:        {stage_a_log_path}")
    print(f"- Stage B candidates: {stage_b_candidates_path}")
    print(f"- Stage B shortlist:  {stage_b_top_path}")
    print(f"- Stage B log:        {stage_b_log_path}")
    print(f"- Promotion top:      {promotion_top_path}")
    print(f"- Promotion log:      {promotion_log_path}")


if __name__ == "__main__":
    main()
